{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r78WYvDiSODe",
        "outputId": "7bcb7878-d166-4717-dd01-de83f1cc4d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ikw2eys5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ikw2eys5\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트\n",
        "import torch\n",
        "import clip\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "izd1FXSnSk2x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 변환 설정\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # CLIP 모델의 입력 크기에 맞게 조정\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),  # CLIP 입력 정규화 값\n",
        "                         (0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "# 테스트 데이터셋 로드\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfVNod6KSku3",
        "outputId": "ae6d2a4a-b768-46db-89eb-54951c4c87b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLIP 모델 로드\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4B-qMT5Skqz",
        "outputId": "441804f5-2bda-4efb-bba3-6e156219f4ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:07<00:00, 49.5MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 클래스 이름\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# 프롬프트 템플릿 리스트\n",
        "prompts = [\n",
        "    'a photo of a {}.',\n",
        "    'an image of a {}.',\n",
        "    'a drawing of a {}.',\n",
        "    'a sketch of a {}.',\n",
        "    'a picture of a {}.',\n",
        "    'a blurry photo of a {}.',\n",
        "    'a black and white photo of a {}.',\n",
        "    'a cartoon {}.',\n",
        "    'a painting of a {}.'\n",
        "]\n"
      ],
      "metadata": {
        "id": "vQplxoOnTO7Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, classes, prompt_template):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        text_inputs = [prompt_template.format(c) for c in classes]\n",
        "        text_tokens = clip.tokenize(text_inputs).to(device)\n",
        "        text_features = model.encode_text(text_tokens)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        for images, labels in tqdm(test_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            image_features = model.encode_image(images)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            logits = (100.0 * image_features @ text_features.T)\n",
        "            probs = logits.softmax(dim=-1)\n",
        "            predictions = torch.argmax(probs, dim=1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "5A926Xs1VgWh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for prompt in prompts:\n",
        "    print(f'프롬프트: \"{prompt}\" 평가 중...')\n",
        "    accuracy = evaluate_model(model, test_loader, classes, prompt)\n",
        "    results[prompt] = accuracy\n",
        "    print(f'정확도: {accuracy:.4f}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AjBta8LVp48",
        "outputId": "78030b93-178f-466d-f79b-7b392192ec4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "프롬프트: \"a photo of a {}.\" 평가 중...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 3/157 [00:33<28:27, 11.09s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프롬프트와 정확도 리스트 생성\n",
        "prompt_list = list(results.keys())\n",
        "accuracy_list = [results[p] for p in prompt_list]\n",
        "\n",
        "# 막대 그래프 그리기\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(prompt_list, accuracy_list, color='skyblue')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.title('CLIP Zero-shot Performance with Different Prompts on CIFAR-10')\n",
        "plt.gca().invert_yaxis()  # 높은 정확도가 위로 오도록 순서 변경\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T2eVLjNlVr1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}